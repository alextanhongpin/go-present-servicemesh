Service Mesh
Build Resilient Services
18:50 19 Apr 2018
Tags: servicemesh, microservices, linkerd

Alex Tan Hong Pin
Developer Advocate, SeekAsia
alextan220990@gmail.com
https://github.com/alextanhongpin

* Evolution of Services

.image assets/evolution.jpg
.caption From monolith to SOA to microservice to serverless

: Get basic understanding on what service mesh is, and how to apply it in your organization

* Hard-Coded Logic or Infrastructure?

When writing services, we need:

- retries and timeouts
- monitoring/visibility
- tracing
- service discovery, etc. 
- TLS

which are all hard-coded into each application.

* Pros/Cons for Code or Infrastructure

Arguments for code:

- more granular control
- easier to implement if working on one language only
- typical in monolith architecture

Arguments for infra:

- language-agnostic (don't need to worry about missing libraries)
- drop-in plug-and-play
- isolate repetitive functionality
- deployable as a container sidecar or per-host (k8s daemonsets)

* What is a Service Mesh

A dedicated infrastructure layer for making service-to-service communication safe, fast and reliable. 

  If you are building a cloud native application, you need a service mesh. - Buoyant

.image assets/mesh.png
.caption Managing complex service topology with service mesh

: Almost every system is a distributed system.

* Linkerd (CNCF)

.image assets/linkerd.png

- Resilient service mesh for cloud native apps. Successor of Finagle that is used by Twitter.
- Alternatives are *Envoy*, *Traefik*, *Istio* and *Conduit* (new!).
- First generation service mesh includes Netflix's [[https://github.com/Netflix/Hystrix][*Hystrix*]] and Facebook's [[https://github.com/facebook/proxygen][*Proxygen*]].
- Part of [[https://www.cncf.io/][Cloud Native Computing Foundation (CNCF)]]

* Capabilities

Linkerd provides the following capabilities, without applications needing to be aware of

- Baseline resilience
- Top-line service metrics & Distributed tracing
- Latency and failure tolerance
- Service discovery
- Protocol upgrades
- Load Balancer
- Dynamic Routing

* Resiliency Patterns by Uwe Friedrichsen

.image assets/resiliency_patterns_by_uwe_friedrichsen.png

* Failures in production

- *Thundering* *herd* *problem* occurs when a large number of processes waiting for an event are awoken when that event occurs, but only one process is able to proceed at a time. After the processes wake up, they all demand the resource and a decision must be made as to which process can continue.
- *Cascading* *failure* is a process in a system of interconnected parts in which the failure of one or few parts can trigger the failure of other parts
- According to Google, 70 percent of outages happens from making changes in production

* Configuration

* Baseline resilience

retry budgets, deadlines, circuit-breaking

  routers:
  - ...
    service:
      retries:
        budget:
          minRetriesPerSec: 5
          percentCanRetry: 0.5
          ttlSecs: 15
        backoff:
          kind: jittered
          minMs: 10
          maxMs: 10000

Top: For every *5* non-retry calls, allow *half* of it to retry. The retry should be a jittered backoff in range of *0.01s* to *10s*. 

* Top-line service metrics & Distributed tracing

Connects with _Prometheus_, _InfluxDB_, _StatsD_ and _Zipkin_, provides success rates, request volumes, and latencies.

Example *Prometheus* config:

  telemetry:
  - kind: io.l5d.prometheus
    path: /admin/metrics/prometheus
    prefix: linkerd_

* Latency and failure tolerance

Failure and latency-aware load balancing that can route around slow or broken service instances.

  routers:
  - ...
    client:
      failureAccrual:
        kind: io.l5d.successRate
        successRate: 0.9
        requests: 1000
        backoff:
          kind: jittered
          minMs: 5000
          maxMs: 300000

Top: Prefer to send requests to services that has a 90% success rate

* Service discovery

Locate destination instances through _namers_. A _namer_ binds a concrete name to a physical address.

- File-based 
- ZooKeeper ServerSets 
- Consul
- Kubernetes
- Marathon
- DNS SRV Records
- ZooKeeper
- Curator

* Protocol upgrades

Wrapping cross-network communication in TLS, or converting HTTP/1.1 to HTTP/2.0 (gRPC)

.image assets/buoyant-l2l-diagram.png

Keeps your code DRY :)

* Load Balancer

Provides latency-aware load balancing:

- *p2c*: Power of Two Choices: Least Loaded
- *ewma*: Power of Two Choices: Peak EWMA
- *aperture*: Aperture: Least Loaded
- *roundRobin*: RoundRobin

Also, supports gRPC load balancing!

Linkerd is categorized as a L7 (application level) load balancer, similar to Envoy. Compared to L3/4 (transport level) load balancers, L7 LB actually adds slightly more latency due to additional hops.

* Dynamic Routing

Route requests between different versions of services, failover between clusters, etc.

This kind of traffic shifting enables things like:

- blue-green deploys
- canary release
- A/B testing 
- cross-DC failover

Using *namerd* enables the ability to make these dtab changes at runtime, without needing to restart Linkerd.

* Why do you need it (Organization Standpoint)

- you have a lot of services
- you want to add resiliency to your services
- you are using more than 1 language
- you want to scale it across teams

: Each services can have multiple instances - service discovery, managing updates

* Demo

* Canary release

*Canary* *release* is a technique to reduce the risk of introducing a new software version in production by slowly rolling out the change to a small subset of users before rolling it out to the entire infrastructure and making it available to everybody.

.image assets/route.png

: The name for this technique originates from miners who would carry a canary in a cage down the coal mines. If toxic gases leaked into the mine, it would kill the canary before killing the miners. A canary release provides a similar form of early warning for potential problems before impacting your entire production infrastructure or user base.

* Traffic Shifting with Linkerd

We will be running k8s with Minikube for this demo

- setup linkerd + namerd
- setup blue & green services
- expose the linkerd port
- shift blue traffic to green from 10%, 50% and then 100% 

* References

.link https://linkerd.io/ Linkerd Website
.link https://www.cncf.io/ Cloud Native Computing Foundation (CNCF)
.link https://grpc.io/blog/loadbalancing Load Balancing gRPC
.link https://github.com/alextanhongpin/kubernetes-structure github.com/alextanhongpin/kubernetes-structure
.link https://github.com/alextanhongpin/go-present-servicemesh Presentation Materials
.link https://www.slideshare.net/ufried/patterns-of-resilience Resiliency Patterns by Uwe Friedrichsen
.link https://www.nginx.com/blog/mitigating-thundering-herd-problem-pbs-nginx/#thunderingherd Mitigating Thundering Herd Problem with Nginx

* Q&A